{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAXvsNumpyModelTraining.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkFFNDUxYHqN0w2mYhk6ng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/LearningJax/blob/main/Notebooks/JAXvsNumpyModelTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvV23UDzeT1Z"
      },
      "source": [
        "# Jax Vs Numpy\n",
        "In this notebook we will compare simple LinearRegression Model Training using Jax vs Numpy based Model and See how it differs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FaW8UUSFTOA"
      },
      "source": [
        "## Step1. Model Traing with Simple Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a82-pgbOFSiK",
        "outputId": "ec125854-b92f-493f-e60c-945fa4a4f7ba"
      },
      "source": [
        "import numpy as np\n",
        "xs = np.random.normal(size=(100,))\n",
        "noise = np.random.normal(scale=0.1, size=(100,))\n",
        "ys = xs * 3 - 1 + noise\n",
        "\n",
        "def model(theta, x):\n",
        "  W, b = theta\n",
        "  return W*x + b\n",
        "  \n",
        "def update(theta, xs, ys, lr=0.1):\n",
        "  # Performing Gradient Descent\n",
        "  W, b = theta\n",
        "  n = float(len(xs)) # Number of elements in X\n",
        "  # n = float(len(xs))\n",
        "  for x, y in zip(xs, ys):\n",
        "    pred = model(theta, x)\n",
        "    D_W = (-2/n) *  (x * (y - pred))  # Derivative wrt W\n",
        "    D_b = (-2/n) *(y - pred)  # Derivative wrt b\n",
        "    W = W - lr * D_W # Update W\n",
        "    b = b - lr * D_b  # Update b\n",
        "  return (W, b)\n",
        "\n",
        "# initialize single weight and bias variable with one\n",
        "theta = np.array([1.,1.])\n",
        "\n",
        "# train model and update weights\n",
        "for _ in range(1000):\n",
        "  theta = update(theta, xs, ys)\n",
        "theta"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 168 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c14p9Nai32ph"
      },
      "source": [
        "Reference:\n",
        "\n",
        "* https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqtTO1PBgtZt"
      },
      "source": [
        "## Step2. Model Training With Jax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KOK1xabeHZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef07f83-d8b9-4ffa-8fc2-166c2835396d"
      },
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "xs = np.random.normal(size=(100,))\n",
        "noise = np.random.normal(scale=0.1, size=(100,))\n",
        "ys = xs * 3 - 1 + noise\n",
        "\n",
        "def model(theta, x):\n",
        "  w, b = theta\n",
        "  return w*x + b\n",
        "\n",
        "def loss_fn(theta, x, y):\n",
        "  # make prediction\n",
        "  prediction = model(theta, x)\n",
        "  return jnp.mean((prediction-y)**2) # calculate loss\n",
        "\n",
        "def update(theta, x, y, lr=0.1):\n",
        "  return theta - lr * grad(loss_fn)(theta, x, y)\n",
        "\n",
        "# initialize single weight and bias variable with one\n",
        "theta = jnp.array([1.,1.])\n",
        "\n",
        "# train model and update weights\n",
        "for _ in range(1000):\n",
        "  theta = update(theta, xs, ys)\n",
        "theta"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 6.09 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaCZGvyrXDwh"
      },
      "source": [
        "## Step3. With Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azIRQiFUPxCj",
        "outputId": "da8709dc-c4a4-44fe-aae8-6cd50e885347"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "xs = np.random.normal(size=(100,))\n",
        "noise = np.random.normal(scale=0.1, size=(100,))\n",
        "ys = xs * 3 - 1 + noise\n",
        "\n",
        "\n",
        "class LinearRegressionModel(torch.nn.Module):\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(LinearRegressionModel, self).__init__()\n",
        "\t\tself.linear = torch.nn.Linear(1, 1) # One in and one out\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\ty_pred = self.linear(x)\n",
        "\t\treturn y_pred\n",
        "\n",
        "# our model\n",
        "our_model = LinearRegressionModel()\n",
        "\n",
        "criterion = torch.nn.MSELoss(size_average = False)\n",
        "optimizer = torch.optim.SGD(our_model.parameters(), lr = 0.1)\n",
        "\n",
        "for epoch in range(1, 1001):\n",
        "  for x, y in zip(xs, ys):\n",
        "\t\t# Forward pass: Compute predicted y by passing\n",
        "\t\t# x to the model\n",
        "    x_data = Variable(torch.Tensor([[x.item()]]))\n",
        "    y_data = Variable(torch.Tensor([[y.item()]]))\n",
        "    pred_y = our_model(x_data)\n",
        "    # Compute and print loss\n",
        "    loss = criterion(pred_y, y_data)\n",
        "\n",
        "    # Zero gradients, perform a backward pass,\n",
        "    # and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  # if epoch%100==0:\n",
        "  #   print('epoch {}, loss {}'.format(epoch, loss.item()))\n",
        "\n",
        "for param in our_model.parameters():\n",
        "  print(param)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[2.9793]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-1.0052], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[2.9508]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.9880], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[3.0377]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.9840], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[3.0175]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.9371], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[2.9787]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-1.0217], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[3.0295]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-1.0188], requires_grad=True)\n",
            "1 loop, best of 5: 20.9 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1XKZeXx38qC"
      },
      "source": [
        "Reference:     \n",
        "* https://www.geeksforgeeks.org/linear-regression-using-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ0I3kwkz-1c",
        "outputId": "8f33b4ed-63a2-4e1c-f59c-a59a6366d150"
      },
      "source": [
        "new_var = Variable(torch.Tensor([[4.0]]))\n",
        "pred_y = our_model(new_var)\n",
        "pred_y"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[11.3519]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruP8FCpR2m5S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}